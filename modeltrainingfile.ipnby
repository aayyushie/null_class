# Article Quality Classification Model Training
# This notebook integrates with the Multi-LLM Article Generator to classify article quality

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import nltk
from textblob import TextBlob
import textstat
import re
import pickle
import warnings
warnings.filterwarnings('ignore')

# Download NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('vader_lexicon', quiet=True)

print("📚 Article Quality Classification Model")
print("="*50)
print("This model classifies articles generated by LLMs into quality categories")
print("Integration with Multi-LLM Article Generator Chatbot")

# ============================
# 1. DATA GENERATION & COLLECTION
# ============================

class ArticleFeatureExtractor:
    """Extract comprehensive features from articles for quality classification"""
    
    def __init__(self):
        self.stopwords = set(nltk.corpus.stopwords.words('english'))
    
    def extract_features(self, text):
        """Extract comprehensive features from article text"""
        features = {}
        
        # Basic text statistics
        words = text.split()
        sentences = nltk.sent_tokenize(text)
        paragraphs = text.split('\n\n')
        
        features['word_count'] = len(words)
        features['sentence_count'] = len(sentences)
        features['paragraph_count'] = len([p for p in paragraphs if p.strip()])
        features['avg_words_per_sentence'] = len(words) / max(len(sentences), 1)
        features['avg_sentences_per_paragraph'] = len(sentences) / max(len([p for p in paragraphs if p.strip()]), 1)
        
        # Readability metrics
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)
        features['automated_readability_index'] = textstat.automated_readability_index(text)
        features['coleman_liau_index'] = textstat.coleman_liau_index(text)
        
        # Lexical diversity
        unique_words = set(word.lower() for word in words if word.isalpha())
        features['lexical_diversity'] = len(unique_words) / max(len(words), 1)
        
        # Sentiment analysis
        blob = TextBlob(text)
        features['sentiment_polarity'] = blob.sentiment.polarity
        features['sentiment_subjectivity'] = blob.sentiment.subjectivity
        
        # Structural features
        features['has_introduction'] = 1 if any(word in text.lower()[:200] for word in ['introduction', 'overview', 'begin']) else 0
        features['has_conclusion'] = 1 if any(word in text.lower()[-200:] for word in ['conclusion', 'summary', 'finally']) else 0
        features['has_headings'] = len(re.findall(r'\n#+\s', text)) + len(re.findall(r'\n[A-Z][^.]*:\n', text))
        
        # Language complexity
        complex_words = [word for word in words if len(word) > 6]
        features['complex_word_ratio'] = len(complex_words) / max(len(words), 1)
        
        # Punctuation and formatting
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['comma_ratio'] = text.count(',') / max(len(words), 1)
        
        return features

def generate_synthetic_articles():
    """Generate synthetic article data with quality labels"""
    print("\n🔄 Generating synthetic article dataset...")
    
    # High-quality article templates
    high_quality_articles = [
        """
        # The Future of Renewable Energy
        
        ## Introduction
        Renewable energy has emerged as a critical solution to global climate change and energy security challenges. As we advance into the 21st century, the transition from fossil fuels to sustainable energy sources represents one of humanity's most significant technological and environmental endeavors.
        
        ## Current State of Renewable Technologies
        Solar photovoltaic technology has experienced remarkable cost reductions, with prices dropping by over 80% in the past decade. Wind energy has similarly become competitive with traditional energy sources, particularly in regions with favorable wind conditions. These developments have accelerated adoption rates globally.
        
        ## Economic Implications
        The renewable energy sector has created millions of jobs worldwide while reducing long-term energy costs for consumers. Government incentives and private investment have catalyzed innovation, leading to improved efficiency and storage solutions.
        
        ## Challenges and Solutions
        Despite significant progress, challenges remain in energy storage, grid integration, and intermittency management. However, advances in battery technology and smart grid systems are addressing these concerns effectively.
        
        ## Conclusion
        The future of renewable energy appears promising, with continued technological advancement and policy support driving sustainable growth. This transition represents not just an environmental necessity but an economic opportunity for nations worldwide.
        """,
        
        """
        # Artificial Intelligence in Healthcare: Transforming Patient Care
        
        ## Overview
        Artificial intelligence is revolutionizing healthcare delivery, offering unprecedented opportunities to improve diagnostic accuracy, treatment outcomes, and operational efficiency. This technological transformation is reshaping how medical professionals approach patient care.
        
        ## Diagnostic Applications
        AI-powered imaging systems can detect abnormalities with accuracy rates exceeding human specialists in certain conditions. Machine learning algorithms analyze medical images, laboratory results, and patient histories to provide comprehensive diagnostic support.
        
        ## Treatment Personalization
        Precision medicine leverages AI to tailor treatments based on individual patient characteristics, genetic profiles, and treatment response patterns. This approach maximizes therapeutic effectiveness while minimizing adverse effects.
        
        ## Operational Benefits
        Healthcare institutions utilize AI for resource optimization, scheduling, and administrative tasks. These applications reduce costs and improve patient satisfaction through streamlined processes and reduced waiting times.
        
        ## Ethical Considerations
        The integration of AI in healthcare raises important questions about data privacy, algorithmic bias, and the human element in medical care. Addressing these concerns is crucial for successful implementation.
        
        ## Future Prospects
        As AI technology continues advancing, we can expect more sophisticated applications in drug discovery, robotic surgery, and predictive healthcare analytics, ultimately leading to better patient outcomes and more efficient healthcare systems.
        """
    ]
    
    # Medium-quality article templates
    medium_quality_articles = [
        """
        # Climate Change Effects
        
        Climate change is a big problem today. It affects many things around the world. Temperature is rising and weather patterns are changing.
        
        The ice caps are melting because of global warming. This causes sea levels to rise. Many animals are losing their homes because of this.
        
        People need to do something about climate change. We can use renewable energy and reduce pollution. Electric cars are better than gasoline cars.
        
        Governments should make policies to help the environment. Companies should also be more responsible about their carbon footprint.
        
        In conclusion, climate change is serious and we need to act now to protect our planet for future generations.
        """,
        
        """
        # Technology in Education
        
        Technology has changed education a lot. Students now use computers and tablets for learning. Online classes became popular during COVID-19.
        
        Digital tools help teachers create better lessons. Students can access information quickly on the internet. Educational apps make learning more fun and interactive.
        
        However, there are some problems with technology in schools. Not all students have access to devices or internet. Some students get distracted by games and social media.
        
        Teachers need training to use new technology effectively. Schools need to update their infrastructure and wifi systems.
        
        Technology in education has both good and bad points, but it's here to stay and will continue changing how we learn.
        """
    ]
    
    # Low-quality article templates  
    low_quality_articles = [
        """
        # cars are good
        
        cars help people go places fast. they have engines and wheels. people drive them every day to work and stores.
        
        there are many types of cars like sedans trucks and suvs. some cars use gas some use electricity. red cars look nice.
        
        parking can be hard in cities. traffic jams are annoying. car insurance costs money.
        
        self driving cars might be the future. they use computers and sensors. this could be safer maybe.
        
        cars are important for transportation.
        """,
        
        """
        # food is important
        
        people need food to live. there are many different foods like pizza burgers and salads. healthy food is better than junk food but junk food tastes good.
        
        restaurants serve food to customers. cooking at home saves money. some people are vegetarian and dont eat meat.
        
        food can be expensive especially organic food. grocery stores sell food. farmers grow vegetables and fruits.
        
        eating too much food makes people fat. not eating enough food is bad too. balanced diet is important.
        
        food gives energy to the body.
        """
    ]
    
    # Create dataset
    articles = []
    labels = []
    
    # Add high-quality articles (label: 2)
    for article in high_quality_articles * 50:  # Replicate for more data
        # Add some variation
        modified_article = article + f" Additional insight: {np.random.choice(['This topic requires further research.', 'These developments are promising.', 'Implementation challenges remain.'])}"
        articles.append(modified_article)
        labels.append(2)  # High quality
    
    # Add medium-quality articles (label: 1)  
    for article in medium_quality_articles * 75:
        articles.append(article)
        labels.append(1)  # Medium quality
    
    # Add low-quality articles (label: 0)
    for article in low_quality_articles * 75:
        articles.append(article)
        labels.append(0)  # Low quality
    
    return articles, labels

# Generate synthetic dataset
articles, labels = generate_synthetic_articles()

print(f"📊 Dataset created: {len(articles)} articles")
print(f"   - High quality (2): {labels.count(2)} articles")
print(f"   - Medium quality (1): {labels.count(1)} articles") 
print(f"   - Low quality (0): {labels.count(0)} articles")

# ============================
# 2. FEATURE EXTRACTION
# ============================

print("\n🔧 Extracting features from articles...")
feature_extractor = ArticleFeatureExtractor()

# Extract features for all articles
feature_data = []
for article in articles:
    features = feature_extractor.extract_features(article)
    feature_data.append(features)

# Convert to DataFrame
df_features = pd.DataFrame(feature_data)
df_features['quality_label'] = labels

print(f"✅ Features extracted: {df_features.shape[1]-1} features per article")
print("\n📈 Feature summary:")
print(df_features.describe())

# ============================
# 3. EXPLORATORY DATA ANALYSIS
# ============================

print("\n📊 Performing Exploratory Data Analysis...")

# Distribution of quality labels
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
label_counts = pd.Series(labels).value_counts().sort_index()
plt.bar(['Low (0)', 'Medium (1)', 'High (2)'], label_counts.values, color=['red', 'orange', 'green'])
plt.title('Distribution of Article Quality Labels')
plt.ylabel('Count')

# Feature correlation with quality
plt.subplot(1, 3, 2)
correlation_with_quality = df_features.corr()['quality_label'].drop('quality_label').sort_values(key=abs, ascending=False)[:10]
plt.barh(range(len(correlation_with_quality)), correlation_with_quality.values)
plt.yticks(range(len(correlation_with_quality)), correlation_with_quality.index)
plt.title('Top 10 Features Correlated with Quality')
plt.xlabel('Correlation Coefficient')

# Word count by quality
plt.subplot(1, 3, 3)
for quality in [0, 1, 2]:
    quality_data = df_features[df_features['quality_label'] == quality]['word_count']
    plt.hist(quality_data, alpha=0.7, label=f'Quality {quality}', bins=20)
plt.title('Word Count Distribution by Quality')
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.legend()

plt.tight_layout()
plt.savefig('eda_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# ============================
# 4. MODEL TRAINING & EVALUATION
# ============================

print("\n🤖 Training Machine Learning Models...")

# Prepare features and target
X = df_features.drop('quality_label', axis=1)
y = df_features['quality_label']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models to evaluate
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'Naive Bayes': MultinomialNB()
}

# Train and evaluate models
model_results = {}
best_model = None
best_accuracy = 0

print("\n📈 Model Performance Comparison:")
print("="*60)

for name, model in models.items():
    print(f"\n🔄 Training {name}...")
    
    # Handle scaling for different models
    if name == 'Naive Bayes':
        # Naive Bayes requires non-negative features
        X_train_model = X_train
        X_test_model = X_test
    else:
        X_train_model = X_train_scaled  
        X_test_model = X_test_scaled
    
    # Train model
    model.fit(X_train_model, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test_model)
    y_pred_proba = model.predict_proba(X_test_model) if hasattr(model, 'predict_proba') else None
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # Store results
    model_results[name] = {
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }
    
    print(f"✅ {name} Results:")
    print(f"   Accuracy: {accuracy:.4f}")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall: {recall:.4f}")
    print(f"   F1-Score: {f1:.4f}")
    
    # Track best model
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = name

print(f"\n🏆 Best Model: {best_model} (Accuracy: {best_accuracy:.4f})")

# ============================
# 5. DETAILED EVALUATION OF BEST MODEL
# ============================

print(f"\n🔍 Detailed Evaluation of {best_model}:")
print("="*50)

best_model_obj = model_results[best_model]['model']
best_predictions = model_results[best_model]['predictions']

# Confusion Matrix
cm = confusion_matrix(y_test, best_predictions)
print("\n📊 Confusion Matrix:")
print(cm)

# Classification Report
print("\n📋 Classification Report:")
print(classification_report(y_test, best_predictions, target_names=['Low Quality', 'Medium Quality', 'High Quality']))

# Visualize Confusion Matrix
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Low', 'Medium', 'High'],
            yticklabels=['Low', 'Medium', 'High'])
plt.title(f'Confusion Matrix - {best_model}')
plt.ylabel('True Quality')
plt.xlabel('Predicted Quality')

# Feature Importance (for tree-based models)
if hasattr(best_model_obj, 'feature_importances_'):
    plt.subplot(1, 2, 2)
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model_obj.feature_importances_
    }).sort_values('importance', ascending=False).head(10)
    
    plt.barh(range(len(feature_importance)), feature_importance['importance'])
    plt.yticks(range(len(feature_importance)), feature_importance['feature'])
    plt.title(f'Top 10 Feature Importance - {best_model}')
    plt.xlabel('Importance')

plt.tight_layout()
plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')
plt.show()

# ============================
# 6. MODEL OPTIMIZATION
# ============================

print(f"\n⚙️ Optimizing {best_model} with GridSearch...")

# Define hyperparameter grids
param_grids = {
    'Random Forest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10]
    },
    'Gradient Boosting': {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7]
    },
    'Logistic Regression': {
        'C': [0.1, 1, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear', 'saga']
    }
}

if best_model in param_grids:
    param_grid = param_grids[best_model]
    
    # Create fresh model instance
    if best_model == 'Random Forest':
        base_model = RandomForestClassifier(random_state=42)
        X_train_opt = X_train_scaled
        X_test_opt = X_test_scaled
    elif best_model == 'Gradient Boosting':
        base_model = GradientBoostingClassifier(random_state=42)
        X_train_opt = X_train_scaled
        X_test_opt = X_test_scaled
    elif best_model == 'Logistic Regression':
        base_model = LogisticRegression(max_iter=1000, random_state=42)
        X_train_opt = X_train_scaled
        X_test_opt = X_test_scaled
    
    # Perform GridSearch
    grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train_opt, y_train)
    
    # Evaluate optimized model
    optimized_model = grid_search.best_estimator_
    optimized_predictions = optimized_model.predict(X_test_opt)
    optimized_accuracy = accuracy_score(y_test, optimized_predictions)
    
    print(f"✅ Optimized {best_model} Results:")
    print(f"   Best Parameters: {grid_search.best_params_}")
    print(f"   Optimized Accuracy: {optimized_accuracy:.4f}")
    print(f"   Improvement: {optimized_accuracy - best_accuracy:.4f}")
    
    # Use optimized model as final model
    final_model = optimized_model
    final_scaler = scaler
    final_accuracy = optimized_accuracy
else:
    final_model = best_model_obj
    final_scaler = scaler
    final_accuracy = best_accuracy

print(f"\n✅ Final Model Accuracy: {final_accuracy:.4f}")

# Check if model meets minimum requirement
if final_accuracy >= 0.70:
    print("🎯 SUCCESS: Model achieves minimum 70% accuracy requirement!")
else:
    print("⚠️  WARNING: Model does not meet 70% accuracy requirement")

# ============================
# 7. SAVE MODEL AND COMPONENTS
# ============================

print("\n💾 Saving trained model and components...")

# Create model package
model_package = {
    'model': final_model,
    'scaler': final_scaler,
    'feature_extractor': feature_extractor,
    'feature_columns': list(X.columns),
    'accuracy': final_accuracy,
    'model_type': best_model,
    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
}

# Save with pickle
with open('article_quality_classifier.pkl', 'wb') as f:
    pickle.dump(model_package, f)

print("✅ Model saved as 'article_quality_classifier.pkl'")

# ============================
# 8. INTEGRATION TEST
# ============================

print("\n🔗 Testing Integration with Article Generator...")

def predict_article_quality(text, model_package):
    """Predict quality of a new article using trained model"""
    
    # Extract features
    features = model_package['feature_extractor'].extract_features(text)
    feature_df = pd.DataFrame([features])
    
    # Ensure feature order matches training
    feature_df = feature_df.reindex(columns=model_package['feature_columns'], fill_value=0)
    
    # Scale features
    features_scaled = model_package['scaler'].transform(feature_df)
    
    # Make prediction
    prediction = model_package['model'].predict(features_scaled)[0]
    probability = model_package['model'].predict_proba(features_scaled)[0]
    
    quality_labels = ['Low Quality', 'Medium Quality', 'High Quality']
    
    return {
        'predicted_quality': quality_labels[prediction],
        'confidence': max(probability),
        'quality_score': prediction,
        'all_probabilities': {
            'Low Quality': probability[0],
            'Medium Quality': probability[1],
            'High Quality': probability[2]
        }
    }

# Test with sample articles
test_articles = [
    "This is a short article. It has few words. Not much detail.",
    """
    # The Impact of Machine Learning on Modern Healthcare
    
    ## Introduction
    Machine learning has revolutionized healthcare by enabling more accurate diagnoses, personalized treatments, and efficient operations. This comprehensive analysis explores the current applications and future potential of ML in medical settings.
    
    ## Current Applications
    Healthcare institutions worldwide are implementing machine learning algorithms for medical imaging analysis, drug discovery, and patient monitoring systems. These technologies have demonstrated significant improvements in accuracy and efficiency.
    
    ## Conclusion
    The integration of machine learning in healthcare represents a paradigm shift toward more precise, personalized, and accessible medical care for patients globally.
    """
]

print("\n🧪 Testing model on sample articles:")
for i, article in enumerate(test_articles, 1):
    result = predict_article_quality(article, model_package)
    print(f"\nTest Article {i}:")
    print(f"   Predicted Quality: {result['predicted_quality']}")
    print(f"   Confidence: {result['confidence']:.3f}")
    print(f"   Quality Score: {result['quality_score']}")

# ============================
# 9. PERFORMANCE SUMMARY
# ============================

print("\n" + "="*60)
print("📈 FINAL PERFORMANCE SUMMARY")
print("="*60)

print(f"🎯 Model Type: {best_model}")
print(f"📊 Final Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)")
print(f"✅ Meets 70% Requirement: {'Yes' if final_accuracy >= 0.70 else 'No'}")
print(f"📁 Model Saved: article_quality_classifier.pkl")
print(f"🔧 Features Used: {len(X.columns)}")
print(f"📚 Training Samples: {len(X_train)}")
print(f"🧪 Test Samples: {len(X_test)}")

print("\n🔗 Integration Ready:")
print("   - Model can classify articles from LLM generators")
print("   - Features extracted automatically from text")
print("   - Returns quality predictions with confidence scores")
print("   - Compatible with Multi-LLM Article Generator Chatbot")

print("\n💡 Usage Example:")
print("```python")
print("import pickle")
print("with open('article_quality_classifier.pkl', 'rb') as f:")
print("    model_package = pickle.load(f)")
print("result = predict_article_quality(article_text, model_package)")
print("print(f'Quality: {result[\"predicted_quality\"]}')") 
print("```")

print("\n✅ Training Complete! Model ready for deployment.")
